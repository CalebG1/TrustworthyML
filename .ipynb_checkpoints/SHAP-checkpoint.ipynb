{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a55f523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/71/v9k425r51tddm6v4ztbmjhy40000gn/T/pip-req-build-p1va44yv\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/71/v9k425r51tddm6v4ztbmjhy40000gn/T/pip-req-build-p1va44yv\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from clip==1.0) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied: torch in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from clip==1.0) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from clip==1.0) (0.17.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch->clip==1.0) (2024.3.1)\n",
      "Requirement already satisfied: numpy in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torchvision->clip==1.0) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=f558d53953ab6ed30fd87175db76055b2f577184891a0f0f9e3ff8966e64e7f6\n",
      "  Stored in directory: /private/var/folders/71/v9k425r51tddm6v4ztbmjhy40000gn/T/pip-ephem-wheel-cache-oxolac6y/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n",
      "Requirement already satisfied: open_clip_torch in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (2.24.0)\n",
      "Requirement already satisfied: torch>=1.9.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from open_clip_torch) (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from open_clip_torch) (0.17.2)\n",
      "Requirement already satisfied: regex in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from open_clip_torch) (2022.7.9)\n",
      "Requirement already satisfied: ftfy in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from open_clip_torch) (6.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from open_clip_torch) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from open_clip_torch) (0.22.2)\n",
      "Requirement already satisfied: sentencepiece in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from open_clip_torch) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from open_clip_torch) (4.25.3)\n",
      "Requirement already satisfied: timm in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from open_clip_torch) (0.9.16)\n",
      "Requirement already satisfied: filelock in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch>=1.9.0->open_clip_torch) (2024.3.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from ftfy->open_clip_torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (6.0)\n",
      "Requirement already satisfied: requests in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from huggingface-hub->open_clip_torch) (2.31.0)\n",
      "Requirement already satisfied: safetensors in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from timm->open_clip_torch) (0.4.3)\n",
      "Requirement already satisfied: numpy in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torchvision->open_clip_torch) (1.24.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torchvision->open_clip_torch) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->open_clip_torch) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.9.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: sentence_transformers in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (2.6.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sentence_transformers) (4.39.3)\n",
      "Requirement already satisfied: tqdm in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sentence_transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sentence_transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sympy in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install open_clip_torch\n",
    "!pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd9ae37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/77/df/b56175c3fb5bc058774bdcf35f5a71cf9c3c5b909f98a1c688eb71cd3b1f/opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/vanikanoria/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.24.3)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da9640c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import Model\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "from sklearn.model_selection import KFold\n",
    "#from classification_models.tfkeras import Classifiers\n",
    "import torch\n",
    "import open_clip\n",
    "import cv2\n",
    "from sentence_transformers import util\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7128f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_prompt = 'a lamp with flowers'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba66db3",
   "metadata": {},
   "source": [
    "sub prompts: \n",
    "\n",
    "    \n",
    "    a lamp\n",
    "    with flowers\n",
    "    lamp with\n",
    "    a lamp flowers\n",
    "    lamp with\n",
    "    a lamp with\n",
    "    lamp flowers\n",
    "    lamp with flowers\n",
    "    with lamp a flowers\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44315b8b",
   "metadata": {},
   "source": [
    "## Calculate similarities of each image with the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ee1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# image processing model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16-plus-240', pretrained=\"laion400m_e32\")\n",
    "model.to(device)\n",
    "def imageEncoder(img):\n",
    "    img1 = Image.fromarray(img).convert('RGB')\n",
    "    img1 = preprocess(img1).unsqueeze(0).to(device)\n",
    "    img1 = model.encode_image(img1)\n",
    "    return img1\n",
    "def generateScore(image1, image2):\n",
    "    test_img = cv2.imread(image1, cv2.IMREAD_UNCHANGED)\n",
    "    data_img = cv2.imread(image2, cv2.IMREAD_UNCHANGED)\n",
    "    img1 = imageEncoder(test_img)\n",
    "    img2 = imageEncoder(data_img)\n",
    "    cos_scores = util.pytorch_cos_sim(img1, img2)\n",
    "    score = round(float(cos_scores[0][0])*100, 2)\n",
    "    return score\n",
    "#similarity Score: 76.77\n",
    "#print(f\"similarity Score: \", round(generateScore(image1, image2), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4527b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_image = 'a_lamp_with_flowers.jpg'\n",
    "sub1 = 'a_lamp.jpg'\n",
    "sub2 = 'a_lamp_flowers.jpg'\n",
    "sub3 = 'with_flowers.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6666540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score of the image from 'a lamp':  62.44\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity Score of the image from 'a lamp': \", round(generateScore(orig_image, sub1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00b5fd01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score of the image from 'a lamp flowers':  69.55\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity Score of the image from 'a lamp flowers': \", round(generateScore(orig_image, sub2), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0942f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score of the image from 'with flowers':  47.72\n"
     ]
    }
   ],
   "source": [
    "print(f\"Similarity Score of the image from 'with flowers': \", round(generateScore(orig_image, sub3), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d085e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "994886ed",
   "metadata": {},
   "source": [
    "\n",
    "## Calculate shap values of prompt on the basis of the similarity metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1b018",
   "metadata": {},
   "source": [
    "## Using the Gradient Explainer: \n",
    "Expected gradients combines ideas from Integrated Gradients, SHAP, and SmoothGrad into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
